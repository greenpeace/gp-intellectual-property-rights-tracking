{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qAUMS3g3m_8N"
      },
      "source": [
        "# Login to gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "colab_type": "code",
        "id": "2_cgtj7_m-Ki",
        "outputId": "a6be9268-d5f2-45cd-897f-5ee5175ef5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&code_challenge=1SlDJ01wxbdDBl9O_rJnLEFxD5YTAiJRuV21LE5f98Y&code_challenge_method=S256&access_type=offline&response_type=code&prompt=select_account\n",
            "\n",
            "\n",
            "Enter verification code: 4/1AFpsBm2HQ2h6DcgOWjN3n7raqsY0aGrNPuC4ing0r8aALadLLfBy4U\n",
            "\n",
            "You are now logged in as [tzetterl@greenpeace.org].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login --no-launch-browser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GfYbh75w26hT"
      },
      "source": [
        "## Set gcloud project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "2POukYuN2-GN",
        "outputId": "81b4746c-4d61-43e7-cb67-67a49726810b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project techlab-coding-team"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CjP7cMQm2QGD"
      },
      "source": [
        "# Install Libraries\n",
        "In running environments this is created by requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Y62twkKn2NeO"
      },
      "outputs": [],
      "source": [
        "!pip install tldextract\n",
        "!pip install firebase-admin\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install pytz\n",
        "!pip install html5lib\n",
        "!pip install tldextract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bO47mIA33i5O"
      },
      "source": [
        "## Create requirement.txt file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N_LS3ZeI3X6_"
      },
      "source": [
        "# Import all dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dvpY6FY611jv"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import re\n",
        "import requests\n",
        "import urllib.request\n",
        "import socket\n",
        "import tldextract\n",
        "import json\n",
        "import base64\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3cTcvwht5VgW"
      },
      "source": [
        "We are using firebase admin, and this section we are setting that up - you would need to have the gcp project name for your GCP project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9jDxfL8G19yi"
      },
      "outputs": [],
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "#PROJECT_NAME = 'os.environ[\"PROJECT_NAME\"]' when publishing cloud function\n",
        "PROJECT_NAME = '<your project name>'\n",
        "\n",
        "# initialize firebase sdk\n",
        "CREDENTIALS = credentials.ApplicationDefault()\n",
        "firebase_admin.initialize_app(CREDENTIALS, {\n",
        "    'projectId': PROJECT_NAME,\n",
        "})\n",
        "\n",
        "# get firestore client\n",
        "db = firestore.client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "73Yarazx7hIM"
      },
      "source": [
        "Create an array that will hold the keywords that are store in firestore, the keywords are the words that will be used to search goole with.\n",
        "\n",
        "Setting up we need to create an array were we will store the search queries, we also setup a few variables that will be used for createing a record for the database.\n",
        "\n",
        "For the url request we are going to make we are defining a header and also how many search items should be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bDWUbzXnWCly"
      },
      "outputs": [],
      "source": [
        "# Keyword Array filled in by daya in Firestore\n",
        "keywords = []\n",
        "\n",
        "def googlecloudbot_http(request):\n",
        "\n",
        "    # Fake Real Browser\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'}\n",
        "\n",
        "    results = 100 # valid options 10, 20, 30, 40, 50, and 100\n",
        "\n",
        "    # Variables used\n",
        "    ip = ''\n",
        "    country = ''\n",
        "    geolat = ''\n",
        "    geolong = ''\n",
        "    title = ''\n",
        "    description = '' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QnL-g1JqWDyZ"
      },
      "source": [
        "To get started we need to read the firestore database to get the search queries. We make a db call to a Firestore collection named searchquery.\n",
        "\n",
        "We then start a for loop to read each document in the collection and only use the document were the column active is true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6t33tX6snCkz"
      },
      "outputs": [],
      "source": [
        "    searchquery_ref = db.collection(u'searchquery')\n",
        "    \n",
        "    for doc in searchquery_ref.where(u'active', u'==', True).stream():\n",
        "        query = u'{}'.format(doc.to_dict()['queryterm'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gJfHQNPXZ-gW"
      },
      "source": [
        "We are vuilding up the url we going to use to call the search providers website, in this case it's Google Search.\n",
        "\n",
        "After that we start a while loop, so we can catch more than 100 search results, we go to the end of the url search.\n",
        "\n",
        "We call the url with the headers.\n",
        "\n",
        "In google you can get 429 which is that you exceeded the rate limit, if you get 429 you have to wait tills your allowed to continue. Using the Google search you don't want to run it every hour. It this case it's just runs once a week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pBBFxgHoZ-Du"
      },
      "outputs": [],
      "source": [
        "        url = 'https://google.com/search?hl=en&q=' + query + '&source=lnms&sa=X' + '&num={}'.format(results)\n",
        "    \n",
        "        while url:\n",
        "            page = requests.get(url, headers=headers)\n",
        "            if page.status_code == 429:\n",
        "                logging.info('Exceeded Rate Limit: {}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UZtYhqLFbdth"
      },
      "source": [
        "After we receive a correct response 200, we will start parsing the data using the beautifulsoup python library.\n",
        "\n",
        "We start a for loop to find the links the the html page that was rreturned using html elements to find the links we are intrested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nbZQgCTNbeV4"
      },
      "outputs": [],
      "source": [
        "            searchsoup = soup(page.text, \"html.parser\")\n",
        "\n",
        "            mydivs = searchsoup.find_all(\"div\", class_=\"g\") # for debugging purpose\n",
        "    \n",
        "            for link in searchsoup.find_all(\"a\"):\n",
        "                # get link details\n",
        "                link_href = link.get('href')\n",
        "                #Find the Title and Description\n",
        "\n",
        "                if \"url?q=\" in link_href and not \"webcache\" in link_href:\n",
        "                    # Try if Link is Active\n",
        "                    try:\n",
        "                        response = requests.get(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0], timeout = 5)\n",
        "                        active = True\n",
        "                        logging.info(\"Link is active\")\n",
        "                    except:\n",
        "                        active = False\n",
        "                        logging.info(\"Link is inactive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b3Pn9fO1cWoD"
      },
      "source": [
        "When we find a link, we unpack information from that link, one key element is the link text. We also cleaning up the link with a regex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aQtXuxfycWQa"
      },
      "outputs": [],
      "source": [
        "                    title = link.text\n",
        "                    title = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))', '', title, flags=re.MULTILINE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Qln7vYpc46P"
      },
      "source": [
        "The next we doing is to read a database collection named searchquerykeywords and we are focus on the documents in that collection that is active. This will be used to check if the title (link text) matches a word in the document. If match we continue with doing a duplicate check, see if the link already exixst. \n",
        "\n",
        "If we do not get match on the word in the title we ignore the link, the same goes for the the duplicate check, if we find a duplicate we ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "swRbddp9c33n"
      },
      "outputs": [],
      "source": [
        "               # Check if Keywords Exixst in Product title\n",
        "                    searchkeywords_ref = db.collection(u'searchquerykeywords')        \n",
        "                    # Request data from Firestore\n",
        "                    for doc in searchkeywords_ref.where(u'active', u'==', True).stream():\n",
        "                        keywords.append(u'{}'.format(doc.to_dict()['querykeywords']))\n",
        "                                \n",
        "                    if any(x in title for x in keywords):\n",
        "\n",
        "                        # Duplicate check\n",
        "                        docsurl = db.collection(u'searchlinks').where(u'url', u'==', link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]).stream()\n",
        "                        if (len(list(docsurl))):\n",
        "                            logging.info(\"URL Exist, we will ignore\")\n",
        "                        else:\n",
        "                            logging.info(\"URL Not found, we will add to databse\")\n",
        "                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2_WQ96-deBNw"
      },
      "source": [
        "If the link(URL) was not found, we start building up the database document with the details we are intrested in. We also determine the hostname ip address of the link, and also try to get the geo data for that link. \n",
        "\n",
        "We also determine the name of the website by getting the name from the url, if the url is www.amazon.com - we get the name amazon from the url.\n",
        "\n",
        "lastly we build the document that we store to the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VlXbVclec3wl"
      },
      "outputs": [],
      "source": [
        "                           # Create a query against the collection\n",
        "                            urllink = urlparse(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]).netloc\n",
        "                            \n",
        "                            # Find out what Country the IP address is from\n",
        "                            hostname = socket.gethostname()\n",
        "                            try:\n",
        "                                ip = socket.gethostbyname(urllink)\n",
        "                                try:\n",
        "                                    with urllib.request.urlopen(\"https://geolocation-db.com/jsonp/\" + ip) as geourl:\n",
        "                                        geodata = geourl.read().decode()\n",
        "                                        geodata = geodata.split(\"(\")[1].strip(\")\")\n",
        "                                        #print(geodata)\n",
        "                                        geodata = json.loads(geodata)\n",
        "                                        country = geodata[\"country_name\"]\n",
        "                                        geolat = geodata[\"latitude\"]\n",
        "                                        geolong = geodata[\"longitude\"]\n",
        "                                except:\n",
        "                                    logging.info(\"Problem getting Country Details\")\n",
        "                            except socket.error:\n",
        "                                logging.info(\"Socket Error IP could not be obtained\")\n",
        "\n",
        "                            # remove www/http or https from url\n",
        "                            # Get the Shop name\n",
        "                            shopurl = tldextract.extract(urllink)\n",
        "                            shop = shopurl.domain\n",
        "\n",
        "                            data = {\n",
        "                                'title': title,\n",
        "                                'description': description,\n",
        "                                'shop': shop,\n",
        "                                'date': _now(), # datetime object containing current date and time\n",
        "                                'url': link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0],\n",
        "                                'country': country,\n",
        "                                'category': 'Google Search',\n",
        "                                'search': query,\n",
        "                                'ip_address': ip,\n",
        "                                'status': active,\n",
        "                                'lat': geolat,\n",
        "                                'long': geolong\n",
        "                            }\n",
        "                            db.collection('searchlinks').document().set(data)  # Add a new doc in collection links with ID shop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q5nGRWcyc3Vw"
      },
      "source": [
        "The end of the code exit any for loop, or while loop is looking if there is another page and if so makes another request to get the next page.\n",
        "\n",
        "When the Cloud Function completes - the cloud function goes to sleep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ugqi7a-Ie-7I"
      },
      "outputs": [],
      "source": [
        "                    else:\n",
        "                        logging.info(\"No match on Keywords\")    # -> <match object>\n",
        "            # get next page url\n",
        "            url = searchsoup.find('a', id='pnnext')\n",
        "            if url:\n",
        "                url = 'https://www.google.com/' + url['href'] + '&source=lnms&sa=X'\n",
        "            else:\n",
        "                logging.info('Search Completed: {}')\n",
        "    #return \"All Done\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cnsCXHpoV_ES"
      },
      "source": [
        "We have a sub function defined to set the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rNuL1etFV_kD"
      },
      "outputs": [],
      "source": [
        "def _now():\n",
        "    return datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%d %H:%M:%S %Z')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IYFJxw1V_9o"
      },
      "source": [
        "Another sub function is used to clean a url, to remove anything in the url after defined charachters so we do not include utm parameters or other marketing or tracking codes in a url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4g1bTIiGWAkh"
      },
      "outputs": [],
      "source": [
        "def cleanurl(url):\n",
        "    matches = re.findall('(.+\\?)([^#]*)(.*)', url)\n",
        "    if len(matches) == 0:\n",
        "        return url\n",
        "    match = matches[0]\n",
        "    query = match[1]\n",
        "    return match[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAyAjMUqaEHG"
      },
      "source": [
        "We convert the urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xpp2-3qiaEe5"
      },
      "outputs": [],
      "source": [
        "def convert(url):\n",
        "    if url.startswith('http://www.'):\n",
        "        return 'http://' + url[len('http://www.'):]\n",
        "    if url.startswith('//www.'):\n",
        "        return 'https://www' + url[len('//www'):]\n",
        "    if url.startswith('//image.'):\n",
        "        return 'https://' + url[len('//'):]\n",
        "    if url.startswith('www.'):\n",
        "        return 'https://' + url[len('www.'):]\n",
        "    if not url.startswith('http://'):\n",
        "        return 'http://' + url\n",
        "    return url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xd4DQlKN6EvO"
      },
      "source": [
        "# Cloud Function\n",
        "\n",
        "In this section I'm showing you how you can deploy your cloud function from a a Colab Notebook.\n",
        "\n",
        "The first part here we are creatating the requirements.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "2KyoyCao3iKo",
        "outputId": "45b5610c-4806-4778-be1c-da578645937c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "firebase-admin==2.11.0\n",
        "beautifulsoup4==4.8.2\n",
        "requests==2.22.0\n",
        "pytz==2019.3\n",
        "html5lib==1.0.1\n",
        "tldextract==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2F4Neel-YQ0Y"
      },
      "source": [
        "Here we are creating the main.py file, to make this work on your project you need to change the project name in your Cloud Function after deployment. You do that by adding a OS variable to your runtime.\n",
        "You can also do it through the gcloud command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "06qmaEbY6EMK"
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import logging\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import re\n",
        "import requests\n",
        "import urllib.request\n",
        "import socket\n",
        "import tldextract\n",
        "import json\n",
        "import os\n",
        "\n",
        "import base64\n",
        "import os\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "PROJECT_NAME = os.environ[\"PROJECT_NAME\"]\n",
        "\n",
        "# initialize firebase sdk\n",
        "CREDENTIALS = credentials.ApplicationDefault()\n",
        "firebase_admin.initialize_app(CREDENTIALS, {\n",
        "    'projectId': PROJECT_NAME,\n",
        "})\n",
        "\n",
        "# get firestore client\n",
        "db = firestore.client()\n",
        "\n",
        "# Keyword Array filled in by daya in Firestore\n",
        "keywords = []\n",
        "\n",
        "def googlecloudbot_http(request):\n",
        "\n",
        "    # Fake Real Browser\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)'}\n",
        "\n",
        "    results = 100 # valid options 10, 20, 30, 40, 50, and 100\n",
        "\n",
        "    # Variables used\n",
        "    ip = ''\n",
        "    country = ''\n",
        "    geolat = ''\n",
        "    geolong = ''\n",
        "    title = ''\n",
        "    description = '' \n",
        "\n",
        "    # Get available proxies\n",
        "    #cworking_proxies = check_proxies()\n",
        "\n",
        "    searchquery_ref = db.collection(u'searchquery')\n",
        "#    for prox in list(working_proxies):\n",
        "    \n",
        "    for doc in searchquery_ref.where(u'active', u'==', True).stream():\n",
        "        query = u'{}'.format(doc.to_dict()['queryterm'])\n",
        "        print(query)\n",
        "        url = 'https://google.com/search?hl=en&q=' + query + '&source=lnms&sa=X' + '&num={}'.format(results)\n",
        "    \n",
        "        while url:\n",
        "            print(url)\n",
        "\n",
        "            page = requests.get(url, headers=headers)\n",
        "            if page.status_code == 429:\n",
        "                logging.info('Exceeded Rate Limit: {}')\n",
        "\n",
        "            searchsoup = soup(page.text, \"html.parser\")\n",
        "            # searchsoup = soup(page.text, \"html5lib\")\n",
        "\n",
        "            mydivs = searchsoup.find_all(\"div\", class_=\"g\") # for debugging purpose\n",
        "            # print(searchsoup.get_text()) # for debugging purpose   \n",
        "    \n",
        "            for link in searchsoup.find_all(\"a\"):\n",
        "                # get link details\n",
        "                link_href = link.get('href')\n",
        "                #Find the Title and Description\n",
        "\n",
        "                if \"url?q=\" in link_href and not \"webcache\" in link_href:\n",
        "                    # Try if Link is Active\n",
        "                    try:\n",
        "                        response = requests.get(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0], timeout = 5)\n",
        "                        active = True\n",
        "                        logging.info(\"Link is active\")\n",
        "                    except:\n",
        "                        active = False\n",
        "                        logging.info(\"Link is inactive\")\n",
        "\n",
        "                    title = link.text\n",
        "                    title = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))', '', title, flags=re.MULTILINE)\n",
        "                    # Check if Keywords Exixst in Product title\n",
        "                    searchkeywords_ref = db.collection(u'searchquerykeywords')        \n",
        "                    # Request data from Firestore\n",
        "                    for doc in searchkeywords_ref.where(u'active', u'==', True).stream():\n",
        "                        keywords.append(u'{}'.format(doc.to_dict()['querykeywords']))\n",
        "                                \n",
        "                    if any(x in title for x in keywords):\n",
        "\n",
        "                        # Duplicate check\n",
        "                        docsurl = db.collection(u'searchlinks').where(u'url', u'==', link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]).stream()\n",
        "                        if (len(list(docsurl))):\n",
        "                            logging.info(\"URL Exist, we will ignore\")\n",
        "                        else:\n",
        "                            logging.info(\"URL Not found, we will add to databse\")\n",
        "                                \n",
        "                            # Create a query against the collection\n",
        "                            urllink = urlparse(link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0]).netloc\n",
        "                            \n",
        "                            # Find out what Country the IP address is from\n",
        "                            hostname = socket.gethostname()\n",
        "                            try:\n",
        "                                ip = socket.gethostbyname(urllink)\n",
        "                                try:\n",
        "                                    with urllib.request.urlopen(\"https://geolocation-db.com/jsonp/\" + ip) as geourl:\n",
        "                                        geodata = geourl.read().decode()\n",
        "                                        geodata = geodata.split(\"(\")[1].strip(\")\")\n",
        "                                        #print(geodata)\n",
        "                                        geodata = json.loads(geodata)\n",
        "                                        country = geodata[\"country_name\"]\n",
        "                                        geolat = geodata[\"latitude\"]\n",
        "                                        geolong = geodata[\"longitude\"]\n",
        "                                except:\n",
        "                                    logging.info(\"Problem getting Country Details\")\n",
        "                            except socket.error:\n",
        "                                logging.info(\"Socket Error IP could not be obtained\")\n",
        "\n",
        "                            # remove www/http or https from url\n",
        "                            # Get the Shop name\n",
        "                            shopurl = tldextract.extract(urllink)\n",
        "                            shop = shopurl.domain\n",
        "\n",
        "                            data = {\n",
        "                                'title': title,\n",
        "                                'description': description,\n",
        "                                'shop': shop,\n",
        "                                'date': _now(), # datetime object containing current date and time\n",
        "                                'url': link.get('href').split(\"?q=\")[1].split(\"&sa=U\")[0],\n",
        "                                'country': country,\n",
        "                                'category': 'Google Search',\n",
        "                                'search': query,\n",
        "                                'ip_address': ip,\n",
        "                                'status': active,\n",
        "                                'lat': geolat,\n",
        "                                'long': geolong\n",
        "                            }\n",
        "                            db.collection('searchlinks').document().set(data)  # Add a new doc in collection links with ID shop\n",
        "                    else:\n",
        "                        logging.info(\"No match on Keywords\")    # -> <match object>\n",
        "            # get next page url\n",
        "            url = searchsoup.find('a', id='pnnext')\n",
        "            if url:\n",
        "                url = 'https://www.google.com/' + url['href'] + '&source=lnms&sa=X'\n",
        "            else:\n",
        "                logging.info('Search Completed: {}')\n",
        "    #return \"All Done\"\n",
        "\n",
        "    # Send a message\n",
        "   # _sendmessage()\n",
        "def _now():\n",
        "    return datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "\n",
        "def cleanurl(url):\n",
        "    matches = re.findall('(.+\\?)([^#]*)(.*)', url)\n",
        "    if len(matches) == 0:\n",
        "        return url\n",
        "    match = matches[0]\n",
        "    query = match[1]\n",
        "    return match[0]\n",
        "\n",
        "def convert(url):\n",
        "    if url.startswith('http://www.'):\n",
        "        return 'http://' + url[len('http://www.'):]\n",
        "    if url.startswith('//www.'):\n",
        "        return 'https://www' + url[len('//www'):]\n",
        "    if url.startswith('//image.'):\n",
        "        return 'https://' + url[len('//'):]\n",
        "    if url.startswith('www.'):\n",
        "        return 'https://' + url[len('www.'):]\n",
        "    if not url.startswith('http://'):\n",
        "        return 'http://' + url\n",
        "    return url\n",
        "\n",
        "googlecloudbot_http('request')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TlHZz9Iw6nrQ"
      },
      "source": [
        "## Deploy Cloud Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1ySnWu4K6m5r"
      },
      "outputs": [],
      "source": [
        "!gcloud functions deploy googlecloudbot_http --region=europe-west1 --memory=256MB --timeout 256 --runtime python310 --set-env-vars PROJECT_NAME=<gcp_project_name> --trigger-http --allow-unauthenticated "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "google search bot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
